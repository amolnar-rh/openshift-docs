// Module included in the following assemblies:
// * scalability_and_performance/ztp-image-based-upgrade.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-image-based-upgrade-creating-resources-with-ztp_{context}"]
= Preparing resources for the image-based upgrade with GitOps ZTP

The {lcao} needs the following resources prepared for the image-based upgrade:

* OADP resources wrapped in a `ConfigMap` object
* Labeled extra manifests

[id="ztp-image-based-upgrade-creating-backup-resources-with-ztp_{context}"]
== Creating OADP resources for the image-based upgrade with GitOps ZTP

You need to prepare your OADP resources to restore your application after an upgrade.

.Prerequisites

* Provision one or more managed clusters with GitOps ZTP.
* Log in as a user with `cluster-admin` privileges.
* Generate a seed image from a compatible seed cluster.
* Create a separate partition on the target cluster for the container images that is shared between stateroots. For more information about, see _Additional resources_.
* Deploy a version of {lcao} that is compatible with the version used with the seed image.
* Install the OADP Operator, the `DataProtectionApplication` CR and its secret on the target cluster.
* Create an S3-compatible storage solution and a ready-to-use bucket with proper credentials configured. For more information, see Additional resources.

.Procedure

. Ensure that your Git repository that you use with the ArgoCD policies application contains the following directory structure:
+
--
[source,terminal]
----
├── source-crs/
│   ├── ibu/
│   │    ├── ImageBasedUpgrade.yaml
│   │    ├── PlatformBackupRestore.yaml
│   │    ├── PlatformBackupRestoreLvms.yaml
├── ...
├── custom-oadp-workload-crs.yaml
├── ibu-upgrade-ranGen.yaml
├── kustomization.yaml
----

[IMPORTANT]
====
The `kustomization.yaml` file must be located in the same directory structure as shown above to reference the `PlatformBackupRestore.yaml` manifest.
====

The `source-crs/ibu/PlatformBackupRestore.yaml` is provided in the ZTP container image.

.PlatformBackupRestore.yaml
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: acm-klusterlet
  annotations:
    lca.openshift.io/apply-label: "apps/v1/deployments/open-cluster-management-agent/klusterlet,v1/secrets/open-cluster-management-agent/bootstrap-hub-kubeconfig,rbac.authorization.k8s.io/v1/clusterroles/klusterlet,v1/serviceaccounts/open-cluster-management-agent/klusterlet,scheduling.k8s.io/v1/priorityclasses/klusterlet-critical,rbac.authorization.k8s.io/v1/clusterroles/open-cluster-management:klusterlet-admin-aggregate-clusterrole,rbac.authorization.k8s.io/v1/clusterrolebindings/klusterlet,operator.open-cluster-management.io/v1/klusterlets/klusterlet,apiextensions.k8s.io/v1/customresourcedefinitions/klusterlets.operator.open-cluster-management.io,v1/secrets/open-cluster-management-agent/open-cluster-management-image-pull-credentials" <1>
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  includedNamespaces:
  - open-cluster-management-agent
  includedClusterScopedResources:
  - klusterlets.operator.open-cluster-management.io
  - clusterroles.rbac.authorization.k8s.io
  - clusterrolebindings.rbac.authorization.k8s.io
  - priorityclasses.scheduling.k8s.io
  includedNamespaceScopedResources:
  - deployments
  - serviceaccounts
  - secrets
  excludedNamespaceScopedResources: []
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: acm-klusterlet
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "1"
spec:
  backupName:
    acm-klusterlet
----
<1> The value must be a list of comma-separated objects in the `group/version/resource/name` format for cluster-scoped resources, or in the `group/version/resource/namespace/name` format for namespace-scoped resources. It must be attached to the related `Backup` CR. 

If you use {lvms} to create persistent volumes, you can use the `source-crs/ibu/PlatformBackupRestoreLvms.yaml` provided in the ZTP container image to back up your {lvms} resources.

.PlatformBackupRestoreLvms.yaml
[source,yaml]
----
---
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: lvmcluster
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
spec:
  includedNamespaces:
  - openshift-storage
  includedNamespaceScopedResources:
  - lvmclusters
  - lvmvolumegroups
  - lvmvolumegroupnodestatuses
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: lvmcluster
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "2"
spec:
  backupName:
    lvmcluster
----

[NOTE]
====
Depending on your {rh-rhacm} configuration, you might need to include the `v1/secrets/open-cluster-management-agent/open-cluster-management-image-pull-credentials` object for backup.
If your `multiclusterHub` CR has `.spec.imagePullSecret` defined and the secret exists on the `open-cluster-management-agent` namespace in your hub cluster, ensure that the object is included in the `lca.openshift.io/apply-label` annotation. If the secret does not exist, you can remove the object from the `apply-label` annotation.
====

[IMPORTANT]
====
To use the `lca.openshift.io/apply-label` annotation for backing up specific resources, the resources listed in the annotation should also be included in the spec section. If the `lca.openshift.io/apply-label` annotation is used in the `Backup` CR, only the resources listed in the annotation will be backed up, even if other resource types are specified in the spec section or not.
====
--

. Create the OADP CRs for your custom applications in the `openshift-adp` namespace.
You can store these CRs in separate YAML files or you can consolidate them into one single file, for example `custom-oadp-workload-crs.yaml`, separated by the `---` directive.
+
--
.Example custom-oadp-workload-crs.yaml file for LSO
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    lca.openshift.io/apply-label: "apiextensions.k8s.io/v1/customresourcedefinitions/test.example.com,security.openshift.io/v1/securitycontextconstraints/test,rbac.authorization.k8s.io/v1/clusterroles/test-role,rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:scc:test"
  name: backup-app-cluster-resources
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  includedClusterScopedResources:
  - customresourcedefinitions
  - securitycontextconstraints
  - clusterrolebindings
  - clusterroles
 excludedClusterScopedResources:
  - Namespace
---
apiVersion: velero.io/v1
kind: Backup
metadata:
  labels:
    velero.io/storage-location: default
  name: test-app
  namespace: openshift-adp
spec:
  includedNamespaces:
  - test
  includedNamespaceScopedResources:
  - secrets
  - persistentvolumeclaims
  - deployments
  - statefulsets
  - configmaps
  - cronjobs
  - services
  - job
  - poddisruptionbudgets
  - <application_custom_resources> <1>
  excludedClusterScopedResources:
  - persistentVolumes
----
<1> Define custom resources for your application.

.Example custom-oadp-workload-crs.yaml file for {LVMS}
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    lca.openshift.io/apply-label: "apiextensions.k8s.io/v1/customresourcedefinitions/test.example.com,security.openshift.io/v1/securitycontextconstraints/test,rbac.authorization.k8s.io/v1/clusterroles/test-role,rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:scc:test"
  name: backup-app-cluster-resources
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  includedClusterScopedResources:
  - customresourcedefinitions
  - securitycontextconstraints
  - clusterrolebindings
  - clusterroles
 excludedClusterScopedResources:
  - Namespace
---
apiVersion: velero.io/v1
kind: Backup
metadata:
  labels:
    velero.io/storage-location: default
  name: test-app
  namespace: openshift-adp
spec:
  includedNamespaces:
  - test
  includedNamespaceScopedResources:
  - secrets
  - persistentvolumeclaims
  - deployments
  - statefulsets
  - configmaps
  - cronjobs
  - services
  - job
  - poddisruptionbudgets
  - <application_custom_resources> <1>
  includedClusterScopedResources:
  - logicalvolumes.topolvm.io
  - persistentVolumes
----
<1> Define custom resources for your application.

[IMPORTANT]
====
The same version of the applications must function on both the current and the target release of {product-title}.
====
--

.. Define the apply order for the OADP Operator in the `Restore` CRs by using the `lca.openshift.io/apply-wave` field:
+
--
.Example OADP CRs for LSO
[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: test-app-cluster-resources
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "2"
spec:
  backupName:
    test-app-cluster-resources
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: test-app
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "3" 
spec:
  backupName:
    test-app
----

.Example OADP CRs for {LVMS}
[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: test-app-cluster-resources
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "3"
spec:
  backupName:
    test-app-cluster-resources
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: test-app
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "4"
spec:
  backupName:
    test-app
  restorePVs: true
  restoreStatus:
    includedResources:
    - logicalvolumes
----

[NOTE]
====
If you do not define the `lca.openshift.io/apply-wave` annotation in the `Backup` or `Restore` CRs, they will be applied together.
====
--

. Create the `oadp-cm` `ConfigMap` object through the `oadp-cm-policy` in a new `PolicyGenTemplate` called `ibu-upgrade-ranGen.yaml`.
+
[source,yaml]
----
[...]
  sourceFiles:
  - fileName: ConfigMapGeneric.yaml
    policyName: "oadp-cm-policy"
    metadata:
      name: oadp-cm
      namespace: openshift-adp
[...]
----

. Create a `kustomization.yaml` with the following content:
+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

generators: <1>
- ibu-upgrade-ranGen.yaml

configMapGenerator: <2>
- files:
  - source-crs/ibu/PlatformBackupRestore.yaml
  # - <workload_oadp_crs>
  name: oadp-cm
  namespace: ztp-group
generatorOptions:
  disableNameSuffixHash: true 


patches: <3>
- target:
    group: policy.open-cluster-management.io
    version: v1
    kind: Policy
    name: group-ibu-oadp-cm-policy
  patch: |-
    - op: replace
      path: /spec/policy-templates/0/objectDefinition/spec/object-templates/0/objectDefinition/data
      value: '{{hub copyConfigMapData "ztp-group" "oadp-cm" hub}}'
----
<1> Generates the `oadp-cm-policy`.
<2> Creates the `oadp-cm` `ConfigMap` object on the hub cluster with `Backup` and `Restore` CRs.
<3> Overrides the data field of `ConfigMap` added in `oadp-cm-policy`. A hub template is used to propagate the `oadp-cm` `ConfigMap` to all target clusters.

. Push the changes to your Git repository.

[id="ztp-image-based-upgrade-labeling-extramanifests-with-ztp_{context}"]
== Labeling extra manifests for the image-based upgrade with GitOps ZTP

The {lcao} only extracts extra manifests that are labeled with the `lca.openshift.io/target-ocp-version: <target_version>` label.

.Prerequisites

* Provision one or more managed clusters with GitOps ZTP.
* Log in as a user with `cluster-admin` privileges.
* Generate a seed image from a compatible seed cluster.
* Create a separate partition on the target cluster for the container images that is shared between stateroots. For more information about, see _Additional resources_.
* Deploy a version of {lcao} that is compatible with the version used with the seed image.

.Procedure

. If you have manifests that you want to be extracted and applied during the upgrade, label them with the `lca.openshift.io/target-ocp-version: <target_version>` label in your existing `PolicyGenTemplate` CR:
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: example-sno
spec:
  bindingRules:
    sites: "example-sno"
    du-profile: "4.15"
  mcp: "master"
  sourceFiles:
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-fh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15" <1>
      spec:
        resourceName: du_fh
        vlan: 140
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-fh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15" <1>
      spec:
        deviceType: netdevice
        isRdma: false
        nicSelector:
          pfNames: ["ens5f0"]
        numVfs: 8
        priority: 10
        resourceName: du_fh
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-mh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15" <1>
      spec:
        resourceName: du_mh
        vlan: 150
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-mh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15" <1>
      spec:
        deviceType: vfio-pci
        isRdma: false
        nicSelector:
          pfNames: ["ens7f0"]
        numVfs: 8
        priority: 10
        resourceName: du_mh
    - fileName: DefaultCatsrc.yaml <2>
      policyName: "config-policy"
      metadata:
        name: default-cat-source
        namespace: openshift-marketplace
        labels:
            lca.openshift.io/target-ocp-version: "4.15" <1>
      spec:
          displayName: default-cat-source
          image: quay.io/example-org/example-catalog:v1
----
<1> Ensure that the `lca.openshift.io/target-ocp-version` label matches either the y-stream or the z-stream of the target {product-title} version that is specified in the `spec.seedImageRef.version` field of the `ImageBasedUpgrade` CR. The {lcao} only applies the CRs that match the specified version.
<2> (Optional) If you do not want to use custom catalog sources, remove this entry.

. Push the changes to your Git repository.

To start the upgrade process, see the _Performing an image-based upgrade with {lcao} and GitOps ZTP_ section.