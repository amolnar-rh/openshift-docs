// Module included in the following assemblies:
// * edge_computing/ztp-image-based-upgrade.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-image-based-upgrade-creating-configmaps-with-ztp_{context}"]
= Creating ConfigMap objects for the image-based upgrade with GitOps ZTP

The Lifecycle Agent needs all your OADP resources, extra manifests, and custom catalog sources wrapped in a `ConfigMap` objects to process them for the image-based upgrade.

.Prerequisites

* Provision one or more managed clusters with GitOps ZTP.
* Log in as a user with `cluster-admin` privileges.
* Generate a seed image from a compatible seed cluster.
* Create a separate partition on the target cluster for the container images that is shared between stateroots. For more information about, see _Additional resources_.
* Deploy a version of {lcao} that is compatible with the version used with the seed image.

.Procedure

. Ensure that you created the `openshift-adp` and `ztp-common` namespaces.
The root policy is created in the `ztp-common` namespace. If you use another namespace, update the `example-oadp-policy.yaml` policy.
The `ConfigMap` object containing the related OADP CRs are copied to this namespace on the applicable spoke clusters.

. Ensure that your Git repository that you use with the ArgoCD policies application contains the following directory structure:
+
--
[source,terminal]
----
├── source-crs/
│   ├── ibu/
│   │    ├── PlatformBackupRestore.yaml
├── ...
├── custom-oadp-workload-crs.yaml
├── example-oadp-policy.yaml
├── kustomization.yaml
----

[IMPORTANT]
====
The `source-crs/ibu/PlatformBackupRestore.yaml` is provided in the ZTP container image. However, the `kustomization.yaml` file must be located in the same directory structure as shown above to reference the `PlatformBackupRestore.yaml` manifest.
====

.PlatformBackupRestore.yaml
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: acm-klusterlet
  annotations:
    lca.openshift.io/apply-label: "apps/v1/deployments/open-cluster-management-agent/klusterlet,v1/secrets/open-cluster-management-agent/bootstrap-hub-kubeconfig,rbac.authorization.k8s.io/v1/clusterroles/klusterlet,v1/serviceaccounts/open-cluster-management-agent/klusterlet,scheduling.k8s.io/v1/priorityclasses/klusterlet-critical,rbac.authorization.k8s.io/v1/clusterroles/open-cluster-management:klusterlet-admin-aggregate-clusterrole,rbac.authorization.k8s.io/v1/clusterrolebindings/klusterlet,operator.open-cluster-management.io/v1/klusterlets/klusterlet,apiextensions.k8s.io/v1/customresourcedefinitions/klusterlets.operator.open-cluster-management.io,v1/secrets/open-cluster-management-agent/open-cluster-management-image-pull-credentials" <1>
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  includedNamespaces:
  - open-cluster-management-agent
  includedClusterScopedResources:
  - klusterlets.operator.open-cluster-management.io
  - clusterroles.rbac.authorization.k8s.io
  - clusterrolebindings.rbac.authorization.k8s.io
  - priorityclasses.scheduling.k8s.io
  includedNamespaceScopedResources:
  - deployments
  - serviceaccounts
  - secrets
  excludedNamespaceScopedResources: []
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: acm-klusterlet
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "1"
spec:
  backupName:
    acm-klusterlet
----
<1> The value must be a list of comma-separated objects in the `group/version/resource/name` format for cluster-scoped resources, or in the `group/version/resource/namespace/name` format for namespace-scoped resources. It must be attached to the related `Backup` CR. 
--

. Create the OADP CRs for your custom applications in the `openshift-adp` namespace. You can store these CRs in separate YAML files or you can consolidate them into one single file, for example `custom-oadp-workload-crs.yaml`, separated by the `---` directive.
+
--
.Example custom-oadp-workload-crs.yaml file
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  labels:
    velero.io/storage-location: default
  name: backup-example-app
  namespace: openshift-adp
spec:
  includedNamespaces:
  - test
  includedNamespaceScopedResources:
  - secrets
  - deployments
  - statefulsets
  excludedClusterScopedResources:
  - persistentVolumes
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-acm-klusterlet 
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
spec:
  backupName:
    backup-acm-klusterlet
----

[IMPORTANT]
====
The same version of the applications must function on both the current and the target release of {product-title}.
====
--

. Define the apply order for the OADP Operator in the `Backup` and `Restore` CRs by using the `lca.openshift.io/apply-wave` field:
+
--
.Example OADP CRs
[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: acm-klusterlet
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "1"
spec:
  backupName:
    backup-acm-klusterlet
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-acm-klusterlet 
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "2"
spec:
  backupName:
    backup-acm-klusterlet
----

[NOTE]
====
If you do not define the `lca.openshift.io/apply-wave` annotation in the `Backup` or `Restore` CRs, they will be applied together.
====
--

. Create a `kustomization.yaml` that appends the information to a new `ConfigMap`.
+
[source,yaml]
----
configMapGenerator:
- files:
  - source-crs/ibu/PlatformBackupRestore.yaml
  - custom-oadp-workload-crs.yaml <1>
  name: oadp-cm-example
  namespace: ztp-common
generatorOptions:
  disableNameSuffixHash: true <2>
----
<1> Add the CRs of your custom applications to include them in the `ConfigMap` generation.
<2> Disables the hash generation at the end of the `ConfigMap` filename which allows the `ConfigMap` file to be overwritten when a new one is generated with the same name.

. Create the `ConfigMap` object:
+
[source,terminal]
----
$ kustomize build ./ -o oadp-cm-example.yaml
----

. Create the `example-oadp-policy.yaml` file.
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSetBinding
metadata:
  name: global
  namespace: ztp-common
spec:
  clusterSet: global
---
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: oadp-cm-policy-placement
  namespace: ztp-common
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchExpressions:
            - key: common
              operator: In
              values:
                - 'true'
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: oadp-cm-policy-placement-binding
  namespace: ztp-common
placementRef:
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
  name: oadp-cm-policy-placement
subjects:
  - apiGroup: policy.open-cluster-management.io
    kind: Policy
    name: oadp-cm-common-policies
---
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: oadp-cm-common-policies
  namespace: ztp-common
  annotations:
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
    policy.open-cluster-management.io/standards: NIST SP 800-53
spec:
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: oadp-cm-policy
        spec:
          namespaceSelector:
            exclude:
              - kube-*
            include:
              - 'openshift-adp'
          remediationAction: inform
          severity: medium
          object-templates:
          - complianceType: mustonlyhave
            objectDefinition:
              kind: ConfigMap
              apiVersion: v1
              metadata:
                name: oadp-cm
                namespace: openshift-adp
              data: '{{hub copyConfigMapData "ztp-common" "oadp-cm" hub}}'
----

. Reference the `example-oadp-policy.yaml` file in the `resources` field in your `kustomization.yaml` file.
+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

configMapGenerator:
- files:
  - source-crs/ibu/PlatformBackupRestore.yaml
  # - custom-oadp-workload-crs.yaml
  name: oadp-cm
  namespace: ztp-common

generatorOptions:
  disableNameSuffixHash: true

resources:
- example-oadp-policy.yaml
----

. Add the `ConfigMap` object to your site `PolicyGenTemplate`.
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "example-site"
  namespace: "ztp-site"
spec:
  bindingRules:
    sites: "example-site"
    du-profile: "latest"
  mcp: "master"
  sourceFiles:
    ...
    - fileName: oadp-cm-example.yaml
      policyName: "config-policy"
----

. Push the chnages to your Git repository.

[id="ztp-image-based-upgrade-prep-stage-gitops_{context}"]
= Moving to the Prep stage of the image-based upgrade with {lcao} and GitOps ZTP

When you deploy the {lcao} on a cluster, an `ImageBasedUpgrade` CR is automatically created. You update this CR to specify the image repository of the seed image and to move through the different stages.

.Procedure

. Label the manifests that you want to be extracted and applied during the upgrade in your existing `PolicyGenTemplate` CR:
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: sno-ibu
spec:
  bindingRules:
    sites: "example-sno"
    du-profile: "4.15.0"
  mcp: "master"
  sourceFiles:
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-fh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15.0" <1>
      spec:
        resourceName: du_fh
        vlan: 140
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-fh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15.0" <1>
      spec:
        deviceType: netdevice
        isRdma: false
        nicSelector:
          pfNames: ["ens5f0"]
        numVfs: 8
        priority: 10
        resourceName: du_fh
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-mh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15.0" <1>
      spec:
        resourceName: du_mh
        vlan: 150
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-mh"
        labels:
          lca.openshift.io/target-ocp-version: "4.15.0" <1>
      spec:
        deviceType: vfio-pci
        isRdma: false
        nicSelector:
          pfNames: ["ens7f0"]
        numVfs: 8
        priority: 10
        resourceName: du_mh
----
<1> Ensure that the `lca.openshift.io/target-ocp-version` label matches either the y-stream or the z-stream of the target {product-title} version that is specified in the `spec.seedImageRef.version` field of the `ImageBasedUpgrade` CR. The {lcao} only applies the CRs that match the specified version.

. Create a `PolicyGenTemplate` CR that contains policies for all the stages.
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: group-ibu
  namespace: "ztp-group"
spec:
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  evaluationInterval: <1>
    compliant: 10s
    noncompliant: 10s
  sourceFiles:
    - fileName: ibu/ImageBasedUpgrade.yaml
      policyName: "prep-policy"
      spec:
        stage: Prep
        seedImageRef: <2>
          version: "4.15.0"
          image: "quay.io/user/lca-seed:4.15.0"
          pullSecretRef:
            name: "<seed_pull_secret>"
        oadpContent: <3>
        - name: "oadp-cm-example"
          namespace: "openshift-adp"
      status:
        conditions:
          - reason: Completed
            status: "True"
            type: PrepCompleted
    - fileName: ibu/ImageBasedUpgrade.yaml
      policyName: "upgrade-policy"
      spec:
        stage: Upgrade
      status:
        conditions:
          - reason: Completed
            status: "True"
            type: UpgradeCompleted
    - fileName: ibu/ImageBasedUpgrade.yaml
      policyName: "finalize-policy"
      spec:
        stage: Idle
      status:
        conditions:
          - status: "True"
            type: Idle
----
<1> The policy evaluation interval for compliant and non-compliant policies. Set them to `10s` to ensure that the policies status accurately reflects the current upgrade status.
<2> Define the seed image, {product-title} version, and pull secret for the upgrade in the `Prep` stage.
<3> Define the OADP `ConfigMap` resources required for backup and restore in the `Prep` stage.

. Commit, and push the created CRs to the Git repository.

.. Verify that the policies are created:
+
--
[source,terminal]
----
$ oc get policies -n spoke1 | grep -E "group-ibu"
----

.Example output
[source,terminal]
----
ztp-group.group-ibu-prep-policy          inform               NonCompliant          31h
ztp-group.group-ibu-upgrade-policy       inform               NonCompliant          31h
ztp-group.group-ibu-finalize-policy      inform               NonCompliant          31h
----
--

. To reflect the target platform version, update the `du-profile` or the corresponding policy-binding label in the `SiteConfig` CR.
+
--
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: SiteConfig
[...]
spec:
  [...]
    clusterLabels:
      du-profile: "4.15.0"
----

[IMPORTANT]
====
Updating the labels to the target platform version unbinds the existing set of policies.
====
--

. Commit and push the updated `SiteConfig` CR to the Git repository.

. When you are ready to move to the `Prep` stage, create the `ClusterGroupUpgrade` CR with the `Prep` and OADP `ConfigMap` policies:
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-ibu-prep
  namespace: default
spec:
  clusters:
  - spoke1
  enable: true
  managedPolicies:
  - oadp-cm-common-policies
  - group-ibu-prep-policy
  remediationStrategy:
    canaries:
      - spoke1
    maxConcurrency: 1
    timeout: 240
----

. Apply the `Prep` policy:
+
[source,terminal]
----
$ oc apply -f cgu-ibu-prep.yml
----

.. Monitor the status and wait for the `cgu-ibu-prep` `ClusterGroupUpgrade` to report `Completed`.
+
--
[source,terminal]
----
$ oc get cgu -n default
----

.Example output
[source,terminal]
----
NAME                    AGE   STATE       DETAILS
cgu-ibu-prep            31h   Completed   All clusters are compliant with all the managed policies
----
--