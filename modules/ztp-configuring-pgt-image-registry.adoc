// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-advanced-policy-config.adoc

:_module-type: PROCEDURE
[id="ztp-configuring-pgt-image-registry_{context}"]
= Configuring the image registry using PolicyGenTemplate CRs

Use `PolicyGenTemplate` CRs to apply the CRs required to configure the image registry and patch the `imageregistry` configuration.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).

* You have logged in as a user with `cluster-admin` privileges.

* You have created a Git repository where you manage your custom site configuration data for use with GitOps Zero Touch Provisioning (ZTP).

* You have configured a disk partition in the managed cluster that you provisioned with GitOps ZTP.

.Procedure

. Configure the storage class, persistent volume claim, persistent volume, and image registry configuration in the appropriate `PolicyGenTemplate` CR. For example, to configure an individual site, use the following YAML:
+
[source,yaml]
----
sourceFiles:
  - fileName: StorageClass.yaml
    policyName: "sc-for-image-registry"
    metadata:
      name: image-registry-sc
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100" <1>
  - fileName: StoragePVC.yaml
    policyName: "pvc-for-image-registry"
    metadata:
      name: image-registry-pvc
      namespace: openshift-image-registry
      annotations:
         ran.openshift.io/ztp-deploy-wave: "100"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 100Gi
      storageClassName: image-registry-sc
      volumeMode: Filesystem
  - fileName: ImageRegistryPV.yaml
    policyName: "pv-for-image-registry"
    metadata:
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100"
  - fileName: ImageRegistryConfig.yaml
    policyName: "config-for-image-registry"
    complianceType: musthave <2>
    metadata:
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100"
    spec:
      storage:
        pvc:
          claim: "image-registry-pvc"
----
<1> Set the appropriate value for `ztp-deploy-wave` depending on whether you are configuring image registries at the site, common, or group level. `ztp-deploy-wave: "100"` is appropriate for an individual site.
<2> Do not set `complianceType: mustlyonlyhave`. `mustlyonlyhave` causes the registry pod deployment to fail.

.Verification

. Check that the `Config` CRD of the group `imageregistry.operator.openshift.io` instance is not reporting errors. Run the following command from the hub cluster.
//need to follow up with SME, what do we need to run, and if possible get an example of an error
+
[source,terminal]
----
$ oc get image.config.openshift.io cluster -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Image
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2021-10-08T19:02:39Z"
  generation: 5
  name: cluster
  resourceVersion: "688678648"
  uid: 0406521b-39c0-4cda-ba75-873697da75a4
spec:
  additionalTrustedCA:
    name: acm-ice
----

. Check that the `PersistentVolumeClaim` on the mananged cluster is populated with data. Run the following command when logged in to the managed cluster:
//need to follow up with SME, what do we need to run and where, what is an example of expected result
+
[source,terminal]
----
$ oc get pv image-registry-sc
----

. Check that the `registry` pod is up and located under the `openshift-image-registry` namespace:

.. Log in to the managed cluster:
+
[source,terminal]
----
$ oc login -u kubeadmin -p <password_from_install_log> https://api-int.<cluster_name>.<base_domain>:6443
----

.. Log in to the image registry:
+
[source,terminal]
----
$ podman login -u kubeadmin -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000
----

. Check that disk partitioning is correctly configured:

.. SSH into the managed cluster by running the following command:
+
[source,terminal]
----
$ oc debug node/sno-1.example.com
----

.. Check for disk partitioning using `lsblk` to list the blocks:
+
[source,terminal]
----
sh-4.4# lsblk
----
+
.Example output
[source,terminal]
----
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda      8:0    0 446.6G  0 disk
  |-sda1   8:1    0     1M  0 part
  |-sda2   8:2    0   127M  0 part
  |-sda3   8:3    0   384M  0 part /boot
  |-sda4   8:4    0 336.3G  0 part /sysroot
  `-sda5   8:5    0 100.1G  0 part /var/imageregistry <1>
sdb      8:16   0 446.6G  0 disk
sr0     11:0    1   104M  0 rom
----
<1> `/var/imageregistry` indicates that you have successfully listed the block.
